{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a1fbd7",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cdc10fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from agent import CardRecognizer\n",
    "from utils.Loader import CardsDataset\n",
    "from utils.evaluator import Evaluator\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc098a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "recognizer = CardRecognizer(csv_file=\"cards.csv\", device=\"cpu\")\n",
    "dataset = CardsDataset(scale=0.6, split=\"test\", csv_file=\"cards.csv\", target=\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dae23e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    image, true_label = dataset.__getitem__(i)\n",
    "    category, suit = recognizer.classify_card(image)\n",
    "    pred_label = f\"{category} of {suit}\"\n",
    "    true_labels.append(str(dataset.decode_label(true_label)))\n",
    "    pred_labels.append(pred_label)\n",
    "\n",
    "df = pd.DataFrame({\"True labels\": true_labels, \"Pred labels\": pred_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a51fad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True labels</th>\n",
       "      <th>Pred labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   True labels  Pred labels\n",
       "0            0            0\n",
       "1            1            1\n",
       "2            2            2\n",
       "3            3            3\n",
       "4            4            4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_labels = df[\"True labels\"].unique().tolist()\n",
    "df[\"Pred labels\"] = df[\"Pred labels\"].apply(lambda x: unique_labels.index(x) if x in unique_labels else -1)\n",
    "df[\"True labels\"] = df[\"True labels\"].apply(lambda x: unique_labels.index(x) if x in unique_labels else -1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64e5eb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save_confusion_matrix(true_labels, pred_labels, save_path, num_parameters):\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.xlabel(\"Etiquetas Predichas\")\n",
    "    plt.ylabel(\"Etiquetas Verdaderas\")\n",
    "    plt.title(f\"Matriz de ConfusiÃ³n. Num.Param{num_parameters}\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    \n",
    "def evaluate_model(y_pred: np.array, y_test: np.array, class_names_str: list):\n",
    "    df_result = pd.DataFrame({\"Prediction\": y_pred, \"GroundTruth\": y_test})\n",
    "    df_result[\"Prediction\"] = df_result[\"Prediction\"].apply(lambda x: class_names_str[x])\n",
    "    df_result[\"GroundTruth\"] = df_result[\"GroundTruth\"].apply(lambda x: class_names_str[x])\n",
    "    return Evaluator.evaluate_classification_metrics(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed58cb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_and_save_confusion_matrix(\n",
    "    df[\"True labels\"].tolist(),\n",
    "    df[\"Pred labels\"].tolist(),\n",
    "    save_path=\"result/confusion_matrix.png\",\n",
    "    num_parameters=recognizer.size()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed222f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = evaluate_model(df[\"Pred labels\"], df[\"True labels\"], range(len(unique_labels)))\n",
    "df_metrics[\"Clase\"] = df_metrics.index.to_series().iloc[0:len(unique_labels)].apply(lambda x: unique_labels[x])\n",
    "df_metrics = df_metrics[[\"Clase\"] + [col for col in df_metrics.columns if col != \"Clase\"]]\n",
    "df_metrics.to_csv(\"result/metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7a4d61",
   "metadata": {},
   "source": [
    "# MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7aff5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.5005, Validation Loss: 0.5024, Validation Precision: 0.8615\n",
      "Epoch 2, Loss: 0.5604, Validation Loss: 0.2303, Validation Precision: 0.9269\n",
      "Epoch 3, Loss: 0.3879, Validation Loss: 0.1712, Validation Precision: 0.9462\n",
      "Epoch 4, Loss: 0.3020, Validation Loss: 0.2076, Validation Precision: 0.9346\n",
      "Epoch 5, Loss: 0.2461, Validation Loss: 0.1098, Validation Precision: 0.9654\n",
      "Epoch 6, Loss: 0.2354, Validation Loss: 0.1256, Validation Precision: 0.9654\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     40\u001b[39m     loss.backward()\n\u001b[32m     41\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     running_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m mobilenet.eval()\n\u001b[32m     45\u001b[39m val_loss = \u001b[32m0.0\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load MobileNet model\n",
    "mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "\n",
    "# Modify the classifier for the number of classes in CardsDataset\n",
    "num_classes = len(unique_labels)\n",
    "mobilenet.classifier[1] = nn.Linear(mobilenet.last_channel, num_classes)\n",
    "\n",
    "# Define transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Apply transformations to the dataset\n",
    "# Create training and validation datasets\n",
    "train_dataset = CardsDataset(scale=1, split=\"train\", csv_file=\"cards.csv\", target=\"labels\", transform=transform, convert=\"RGB\")\n",
    "valid_dataset = CardsDataset(scale=1, split=\"valid\", csv_file=\"cards.csv\", target=\"labels\", transform=transform, convert=\"RGB\")\n",
    "\n",
    "# Create dataloaders for training and validation\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=12, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=12, shuffle=False)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(mobilenet.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fine-tune the model\n",
    "mobilenet.to(DEVICE)\n",
    "mobilenet.train()\n",
    "for epoch in range(5):  # Number of epochs\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mobilenet(images.to(DEVICE))\n",
    "        loss = criterion(outputs, labels.to(DEVICE).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    mobilenet.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_dataloader:\n",
    "            outputs = mobilenet(images.to(DEVICE))\n",
    "            loss = criterion(outputs, labels.to(DEVICE).float())\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted.to(DEVICE) == torch.argmax(labels.to(DEVICE), dim=1)).sum().item()\n",
    "    val_loss_avg = val_loss / len(valid_dataloader)\n",
    "    val_precision = val_correct / val_total if val_total > 0 else 0\n",
    "    mobilenet.train()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_dataloader):.4f}, Validation Loss: {val_loss_avg:.4f}, Validation Precision: {val_precision:.4f}\")\n",
    "\n",
    "print(\"Number of parameters in the model:\", sum(p.numel() for p in mobilenet.parameters()))\n",
    "\n",
    "test_dataset = CardsDataset(scale=1, split=\"test\", csv_file=\"cards.csv\", target=\"labels\", transform=transform, convert=\"RGB\")\n",
    "\n",
    "mobilenet.eval()\n",
    "\n",
    "pred_labels = []\n",
    "true_labels = []\n",
    "for images, labels in test_dataset:\n",
    "    outputs = mobilenet(images.unsqueeze(0).to(DEVICE))\n",
    "    pred_labels.append(test_dataset.decode_label(outputs.detach().cpu()))\n",
    "    true_labels.append(test_dataset.decode_label(labels.detach().cpu()))\n",
    "\n",
    "print(\"Pred labels:\", len(pred_labels))\n",
    "print(\"True labels:\", len(true_labels))\n",
    "df = pd.DataFrame({\"True labels\": true_labels, \"Pred labels\": pred_labels})\n",
    "\n",
    "unique_labels = df[\"True labels\"].unique().tolist()\n",
    "df[\"Pred labels\"] = df[\"Pred labels\"].apply(lambda x: unique_labels.index(x) if x in unique_labels else -1)\n",
    "df[\"True labels\"] = df[\"True labels\"].apply(lambda x: unique_labels.index(x) if x in unique_labels else -1)\n",
    "\n",
    "df_metrics = evaluate_model(df[\"Pred labels\"], df[\"True labels\"], range(len(unique_labels)))\n",
    "df_metrics[\"Clase\"] = df_metrics.index.to_series().iloc[0:len(unique_labels)].apply(lambda x: unique_labels[x])\n",
    "df_metrics = df_metrics[[\"Clase\"] + [col for col in df_metrics.columns if col != \"Clase\"]]\n",
    "df_metrics.to_csv(\"result/mobilnet_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ae4789",
   "metadata": {},
   "source": [
    "# Vit-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a6d2e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 3.69 GiB of which 2.88 MiB is free. Process 92786 has 946.00 MiB memory in use. Including non-PyTorch memory, this process has 2.74 GiB memory in use. Of the allocated memory 2.25 GiB is allocated by PyTorch, and 396.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m criterion = nn.CrossEntropyLoss()\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# 5. Training loop\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43mvit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m5\u001b[39m):\n\u001b[32m     43\u001b[39m     vit.train()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venvs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1355\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1352\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1353\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venvs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venvs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 915 (2 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venvs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venvs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:942\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    943\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    945\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/venvs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1341\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1334\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1335\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1336\u001b[39m             device,\n\u001b[32m   1337\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1338\u001b[39m             non_blocking,\n\u001b[32m   1339\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1340\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1344\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 3.69 GiB of which 2.88 MiB is free. Process 92786 has 946.00 MiB memory in use. Including non-PyTorch memory, this process has 2.74 GiB memory in use. Of the allocated memory 2.25 GiB is allocated by PyTorch, and 396.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Load a pretrained ViT-B/16 and replace its head\n",
    "weights = ViT_B_16_Weights.IMAGENET1K_V1\n",
    "vit = vit_b_16(weights=weights)\n",
    "\n",
    "# Number of classes\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "# Replace the classification head\n",
    "in_features = vit.heads.head.in_features\n",
    "vit.heads.head = nn.Linear(in_features, num_classes)\n",
    "\n",
    "# 2. Define transforms (use the ViTâspecific mean/std from the weights metadata)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 3. Prepare datasets & dataloaders\n",
    "train_ds = CardsDataset(scale=1, split=\"train\", csv_file=\"cards.csv\",\n",
    "                        target=\"labels\", transform=transform, convert=\"RGB\")\n",
    "valid_ds = CardsDataset(scale=1, split=\"valid\", csv_file=\"cards.csv\",\n",
    "                        target=\"labels\", transform=transform, convert=\"RGB\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=100, shuffle=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=100, shuffle=False)\n",
    "\n",
    "# 4. Optimizer & loss\n",
    "optimizer = optim.Adam(vit.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 5. Training loop\n",
    "vit.to(DEVICE)\n",
    "for epoch in range(5):\n",
    "    vit.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vit(images.to(DEVICE))\n",
    "        loss = criterion(outputs, labels.to(DEVICE).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    vit.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_dataloader:\n",
    "            outputs = vit(images.to(DEVICE))\n",
    "            loss = criterion(outputs, labels.to(DEVICE).float())\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted.to(DEVICE) == torch.argmax(labels.to(DEVICE), dim=1)).sum().item()\n",
    "    val_loss_avg = val_loss / len(valid_dataloader)\n",
    "    val_precision = val_correct / val_total if val_total > 0 else 0\n",
    "    vit.train()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_dataloader):.4f}, Validation Loss: {val_loss_avg:.4f}, Validation Precision: {val_precision:.4f}\")\n",
    "\n",
    "print(\"Number of parameters in the model:\", sum(p.numel() for p in vit.parameters()))\n",
    "\n",
    "test_dataset = CardsDataset(scale=1, split=\"test\", csv_file=\"cards.csv\", target=\"labels\", transform=transform, convert=\"RGB\")\n",
    "\n",
    "vit.eval()\n",
    "\n",
    "pred_labels = []\n",
    "true_labels = []\n",
    "for images, labels in test_dataset:\n",
    "    outputs = vit(images.unsqueeze(0).to(DEVICE))\n",
    "    pred_labels.append(test_dataset.decode_label(outputs.detach().cpu()))\n",
    "    true_labels.append(test_dataset.decode_label(labels.detach().cpu()))\n",
    "\n",
    "print(\"Pred labels:\", len(pred_labels))\n",
    "print(\"True labels:\", len(true_labels))\n",
    "df = pd.DataFrame({\"True labels\": true_labels, \"Pred labels\": pred_labels})\n",
    "\n",
    "unique_labels = df[\"True labels\"].unique().tolist()\n",
    "df[\"Pred labels\"] = df[\"Pred labels\"].apply(lambda x: unique_labels.index(x) if x in unique_labels else -1)\n",
    "df[\"True labels\"] = df[\"True labels\"].apply(lambda x: unique_labels.index(x) if x in unique_labels else -1)\n",
    "\n",
    "df_metrics = evaluate_model(df[\"Pred labels\"], df[\"True labels\"], range(len(unique_labels)))\n",
    "df_metrics[\"Clase\"] = df_metrics.index.to_series().iloc[0:len(unique_labels)].apply(lambda x: unique_labels[x])\n",
    "df_metrics = df_metrics[[\"Clase\"] + [col for col in df_metrics.columns if col != \"Clase\"]]\n",
    "df_metrics.to_csv(\"result/vit_metrics.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
